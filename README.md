# Papers of RLHF

Related papers for RLHF. I summarized the papers based on https://github.com/opendilab/awesome-RLHF and made my comment.

**Contact** : [Ke Sun](https://sites.google.com/view/kesun), ksun6@ualberta.ca

## 2024

* [Reinforcement Learning from Human Feedback with Active Queries](https://arxiv.org/pdf/2402.09401.pdf)
> This paper formulates RLHF as a contextual dueling bandit problem and incorporates active learning in PPO and DPO with detailed regret bounds/query complexity. It achieves similar performance, making half of queries from human preference.



## 2023

* [123](https://arxiv.org/pdf/2110.03155.pdf) 
