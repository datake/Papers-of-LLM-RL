# Papers of RLHF

This is a collection of papers in Reinforcement Learning with Human Feedback (RLHF). I summarized papers I valued and made my comments.

**Contact** : [Ke Sun](https://sites.google.com/view/kesun), ksun6@ualberta.ca

## 2024

* [Reinforcement Learning from Human Feedback with Active Queries](https://arxiv.org/pdf/2402.09401.pdf)
> This paper formulates RLHF as a contextual dueling bandit problem and incorporates active learning in PPO and DPO with detailed regret bounds/query complexity. It achieves similar performance, making half of queries from human preference.



## 2023

* [123](https://arxiv.org/pdf/2110.03155.pdf) 



## Reference

https://github.com/opendilab/awesome-RLHF
